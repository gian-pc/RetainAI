{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Notebook 04: Data Transformation\n",
    "\n",
    "**Autor:** Gian  \n",
    "**Fecha:** 2026-01-19  \n",
    "**Objetivo:** Transformar el dataset limpio para prepararlo para el modelado  \n",
    "\n",
    "---\n",
    "\n",
    "## üìã Contenido\n",
    "\n",
    "1. Configuraci√≥n del entorno\n",
    "2. Carga de datos limpios\n",
    "3. Encoding de variables categ√≥ricas\n",
    "4. Escalado de variables num√©ricas\n",
    "5. Transformaciones matem√°ticas\n",
    "6. Generaci√≥n de dataset transformado\n",
    "7. Reporte de transformaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuraci√≥n del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:15:41.514007Z",
     "start_time": "2026-01-19T17:15:41.503977Z"
    }
   },
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "# Configuraci√≥n\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 8)\n",
    "\n",
    "# Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as importadas correctamente\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Carga de Datos Limpios"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:15:41.611110Z",
     "start_time": "2026-01-19T17:15:41.523456Z"
    }
   },
   "source": [
    "# Rutas\n",
    "OUTPUT_PATH = Path(\"../../outputs/gian\")\n",
    "CLEAN_DATA_PATH = OUTPUT_PATH / \"data\" / \"data_clean.csv\"\n",
    "TRANSFORMED_DATA_PATH = OUTPUT_PATH / \"data\"\n",
    "\n",
    "# Cargar datos limpios\n",
    "df = pd.read_csv(CLEAN_DATA_PATH)\n",
    "\n",
    "print(f\"‚úÖ Dataset cargado: {df.shape[0]:,} registros √ó {df.shape[1]} columnas\")\n",
    "print(f\"üíæ Memoria: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset cargado: 9,701 registros √ó 67 columnas\n",
      "üíæ Memoria: 18.71 MB\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:15:41.622096Z",
     "start_time": "2026-01-19T17:15:41.616697Z"
    }
   },
   "source": [
    "# Crear copia para transformaci√≥n\n",
    "df_transformed = df.copy()\n",
    "\n",
    "print(f\"‚úÖ Copia creada para transformaci√≥n\")\n",
    "print(f\"üìä Shape: {df_transformed.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Copia creada para transformaci√≥n\n",
      "üìä Shape: (9701, 67)\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Encoding de Variables Categ√≥ricas\n",
    "\n",
    "### 3.1. Identificar Variables Categ√≥ricas"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:15:41.640600Z",
     "start_time": "2026-01-19T17:15:41.627382Z"
    }
   },
   "source": [
    "# Identificar columnas categ√≥ricas\n",
    "categorical_cols = df_transformed.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# Excluir columnas que no necesitan encoding\n",
    "exclude_cols = [\n",
    "    \"cliente_id\",  # ID √∫nico\n",
    "    \"pais\", \"ciudad\", \"borough\", \"estado\",  # Ubicaciones (no usar para modelado)\n",
    "    \"fecha_registro\", \"fecha_ultimo_pago\", \"ultimo_contacto_soporte\"  # Fechas (se procesar√°n despu√©s)\n",
    "]\n",
    "categorical_cols = [col for col in categorical_cols if col not in exclude_cols]\n",
    "\n",
    "print(f\"üìä Variables categ√≥ricas a encodear: {len(categorical_cols)}\")\n",
    "print()\n",
    "for i, col in enumerate(categorical_cols, 1):\n",
    "    unique_vals = df_transformed[col].nunique()\n",
    "    print(f\"  {i:2d}. {col:30s} - {unique_vals} categor√≠as\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Variables categ√≥ricas a encodear: 23\n",
      "\n",
      "   1. genero                         - 2 categor√≠as\n",
      "   2. segmento_cliente               - 3 categor√≠as\n",
      "   3. tiene_pareja                   - 2 categor√≠as\n",
      "   4. tiene_dependientes             - 2 categor√≠as\n",
      "   5. tipo_contrato                  - 3 categor√≠as\n",
      "   6. metodo_pago                    - 4 categor√≠as\n",
      "   7. canal_registro                 - 1 categor√≠as\n",
      "   8. descuento_aplicado             - 5 categor√≠as\n",
      "   9. aumento_precio_3m              - 2 categor√≠as\n",
      "  10. facturacion_sin_papel          - 2 categor√≠as\n",
      "  11. servicio_telefono              - 2 categor√≠as\n",
      "  12. lineas_multiples               - 3 categor√≠as\n",
      "  13. tipo_internet                  - 3 categor√≠as\n",
      "  14. seguridad_online               - 3 categor√≠as\n",
      "  15. respaldo_online                - 3 categor√≠as\n",
      "  16. proteccion_dispositivo         - 3 categor√≠as\n",
      "  17. soporte_tecnico                - 3 categor√≠as\n",
      "  18. streaming_tv                   - 3 categor√≠as\n",
      "  19. streaming_peliculas            - 3 categor√≠as\n",
      "  20. nivel_riesgo                   - 5 categor√≠as\n",
      "  21. tipo_queja                     - 6 categor√≠as\n",
      "  22. respuesta_encuesta             - 8 categor√≠as\n",
      "  23. precio_vs_mercado              - 3 categor√≠as\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Label Encoding para Variables Binarias"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:15:41.664089Z",
     "start_time": "2026-01-19T17:15:41.648071Z"
    }
   },
   "source": [
    "# Variables binarias (Si/No, Masculino/Femenino, etc.)\n",
    "binary_cols = []\n",
    "for col in categorical_cols:\n",
    "    if df_transformed[col].nunique() == 2:\n",
    "        binary_cols.append(col)\n",
    "\n",
    "print(f\"üîß Aplicando Label Encoding a {len(binary_cols)} variables binarias...\")\n",
    "print()\n",
    "\n",
    "label_encoders = {}\n",
    "for col in binary_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_transformed[col + \"_encoded\"] = le.fit_transform(df_transformed[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"  ‚úÖ {col}: {list(le.classes_)} ‚Üí {list(range(len(le.classes_)))}\")\n",
    "\n",
    "print()\n",
    "print(f\"‚úÖ {len(binary_cols)} variables binarias encodeadas\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Aplicando Label Encoding a 6 variables binarias...\n",
      "\n",
      "  ‚úÖ genero: ['Femenino', 'Masculino'] ‚Üí [0, 1]\n",
      "  ‚úÖ tiene_pareja: ['No', 'Si'] ‚Üí [0, 1]\n",
      "  ‚úÖ tiene_dependientes: ['No', 'Si'] ‚Üí [0, 1]\n",
      "  ‚úÖ aumento_precio_3m: ['No', 'Si'] ‚Üí [0, 1]\n",
      "  ‚úÖ facturacion_sin_papel: ['No', 'Si'] ‚Üí [0, 1]\n",
      "  ‚úÖ servicio_telefono: ['No', 'Si'] ‚Üí [0, 1]\n",
      "\n",
      "‚úÖ 6 variables binarias encodeadas\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables con m√°s de 2 categor√≠as\n",
    "multi_categorical_cols = [col for col in categorical_cols if col not in binary_cols]\n",
    "\n",
    "# ‚ö†Ô∏è EXCLUIR columnas de fechas (ya est√°n en formato datetime)\n",
    "date_cols = [\"fecha_registro\", \"fecha_ultimo_pago\", \"ultimo_contacto_soporte\"]\n",
    "multi_categorical_cols = [col for col in multi_categorical_cols if col not in date_cols]\n",
    "\n",
    "print(f\"üîß Aplicando One-Hot Encoding a {len(multi_categorical_cols)} variables...\")\n",
    "print()\n",
    "print(\"Variables a encodear:\")\n",
    "for col in multi_categorical_cols:\n",
    "    print(f\"  - {col}: {df_transformed[col].nunique()} categor√≠as\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:15:41.687427Z",
     "start_time": "2026-01-19T17:15:41.668189Z"
    }
   },
   "source": [
    "# Variables con m√°s de 2 categor√≠as\n",
    "multi_categorical_cols = [col for col in categorical_cols if col not in binary_cols]\n",
    "\n",
    "print(f\"üîß Aplicando One-Hot Encoding a {len(multi_categorical_cols)} variables...\")\n",
    "print()\n",
    "\n",
    "# Aplicar One-Hot Encoding\n",
    "df_transformed = pd.get_dummies(df_transformed, \n",
    "                                 columns=multi_categorical_cols,\n",
    "                                 prefix=multi_categorical_cols,\n",
    "                                 drop_first=True)  # Evitar multicolinealidad\n",
    "\n",
    "print(f\"‚úÖ One-Hot Encoding aplicado\")\n",
    "print(f\"üìä Nuevas columnas creadas: {df_transformed.shape[1] - df.shape[1]}\")\n",
    "print(f\"üìä Shape actual: {df_transformed.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Aplicando One-Hot Encoding a 17 variables...\n",
      "\n",
      "‚úÖ One-Hot Encoding aplicado\n",
      "üìä Nuevas columnas creadas: 34\n",
      "üìä Shape actual: (9701, 101)\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Escalado de Variables Num√©ricas\n",
    "\n",
    "### 4.1. Identificar Variables Num√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:15:41.695493Z",
     "start_time": "2026-01-19T17:15:41.691483Z"
    }
   },
   "source": [
    "# Identificar columnas num√©ricas\n",
    "numeric_cols = df_transformed.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "# Excluir columnas que no necesitan escalado\n",
    "exclude_numeric = [\"cliente_id\", \"cancelacion\", \"es_mayor\", \"codigo_postal\",\n",
    "                   \"latitud\", \"longitud\"]  # IDs, target, flags, coordenadas\n",
    "\n",
    "# Tambi√©n excluir las columnas encoded que acabamos de crear\n",
    "exclude_numeric.extend([col + \"_encoded\" for col in binary_cols])\n",
    "\n",
    "numeric_cols = [col for col in numeric_cols if col not in exclude_numeric]\n",
    "\n",
    "print(f\"üìä Variables num√©ricas a escalar: {len(numeric_cols)}\")\n",
    "print()\n",
    "for i, col in enumerate(numeric_cols[:10], 1):  # Mostrar solo las primeras 10\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "if len(numeric_cols) > 10:\n",
    "    print(f\"  ... y {len(numeric_cols) - 10} m√°s\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Variables num√©ricas a escalar: 31\n",
      "\n",
      "   1. edad\n",
      "   2. ingreso_mediano\n",
      "   3. densidad_poblacional\n",
      "   4. antiguedad\n",
      "   5. cargo_mensual\n",
      "   6. ingresos_totales\n",
      "   7. errores_pago\n",
      "   8. score_riesgo\n",
      "   9. conexiones_mensuales\n",
      "  10. dias_activos_semanales\n",
      "  ... y 21 m√°s\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Aplicar StandardScaler (Z-score normalization)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:15:41.711867Z",
     "start_time": "2026-01-19T17:15:41.702861Z"
    }
   },
   "source": [
    "# Aplicar StandardScaler\n",
    "print(\"üîß Aplicando StandardScaler (media=0, std=1)...\")\n",
    "print()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_transformed[numeric_cols] = scaler.fit_transform(df_transformed[numeric_cols])\n",
    "\n",
    "print(\"‚úÖ StandardScaler aplicado\")\n",
    "print()\n",
    "print(\"üìä Verificaci√≥n (primeras 5 columnas):\")\n",
    "for col in numeric_cols[:5]:\n",
    "    mean = df_transformed[col].mean()\n",
    "    std = df_transformed[col].std()\n",
    "    print(f\"  {col:30s} - Media: {mean:7.4f}, Std: {std:7.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Aplicando StandardScaler (media=0, std=1)...\n",
      "\n",
      "‚úÖ StandardScaler aplicado\n",
      "\n",
      "üìä Verificaci√≥n (primeras 5 columnas):\n",
      "  edad                           - Media: -0.0000, Std:  1.0001\n",
      "  ingreso_mediano                - Media:  0.0000, Std:  1.0001\n",
      "  densidad_poblacional           - Media: -0.0000, Std:  1.0001\n",
      "  antiguedad                     - Media:  0.0000, Std:  1.0001\n",
      "  cargo_mensual                  - Media:  0.0000, Std:  1.0001\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Transformaciones Matem√°ticas\n",
    "\n",
    "### 5.1. Identificar Variables Sesgadas"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:15:41.724714Z",
     "start_time": "2026-01-19T17:15:41.715507Z"
    }
   },
   "source": [
    "# Calcular skewness de variables num√©ricas ANTES de escalar\n",
    "# (usamos el dataset original para esto)\n",
    "print(\"üìä Analizando sesgo (skewness) de variables num√©ricas...\")\n",
    "print()\n",
    "\n",
    "skewness = df[numeric_cols].skew().sort_values(ascending=False)\n",
    "highly_skewed = skewness[abs(skewness) > 1].index.tolist()\n",
    "\n",
    "print(f\"Variables con alto sesgo (|skew| > 1): {len(highly_skewed)}\")\n",
    "print()\n",
    "for col in highly_skewed[:10]:  # Mostrar las 10 m√°s sesgadas\n",
    "    print(f\"  {col:30s} - Skewness: {skewness[col]:7.2f}\")\n",
    "\n",
    "print()\n",
    "print(\"üí° Nota: Estas variables podr√≠an beneficiarse de transformaci√≥n log\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analizando sesgo (skewness) de variables num√©ricas...\n",
      "\n",
      "Variables con alto sesgo (|skew| > 1): 10\n",
      "\n",
      "  errores_pago                   - Skewness:    4.22\n",
      "  escaladas                      - Skewness:    3.91\n",
      "  tickets_soporte                - Skewness:    2.96\n",
      "  dias_ultima_conexion           - Skewness:    2.48\n",
      "  referencias_hechas             - Skewness:    2.06\n",
      "  score_riesgo                   - Skewness:    1.63\n",
      "  downgrade_reciente             - Skewness:    1.40\n",
      "  ingresos_totales               - Skewness:    1.36\n",
      "  dias_mora                      - Skewness:    1.23\n",
      "  cambio_plan_reciente           - Skewness:    1.12\n",
      "\n",
      "üí° Nota: Estas variables podr√≠an beneficiarse de transformaci√≥n log\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Aplicar Transformaci√≥n Log (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:15:41.732135Z",
     "start_time": "2026-01-19T17:15:41.729252Z"
    }
   },
   "source": [
    "# NOTA: Esta secci√≥n es opcional y se puede aplicar en Feature Engineering\n",
    "# Por ahora, solo documentamos las variables que podr√≠an beneficiarse\n",
    "\n",
    "print(\"üìù Variables candidatas para transformaci√≥n log:\")\n",
    "print()\n",
    "for col in highly_skewed[:5]:\n",
    "    print(f\"  - {col}\")\n",
    "print()\n",
    "print(\"üí° Estas transformaciones se aplicar√°n en el Notebook 06 (Feature Engineering)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Variables candidatas para transformaci√≥n log:\n",
      "\n",
      "  - errores_pago\n",
      "  - escaladas\n",
      "  - tickets_soporte\n",
      "  - dias_ultima_conexion\n",
      "  - referencias_hechas\n",
      "\n",
      "üí° Estas transformaciones se aplicar√°n en el Notebook 06 (Feature Engineering)\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Generaci√≥n de Dataset Transformado"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:15:42.037739Z",
     "start_time": "2026-01-19T17:15:41.738937Z"
    }
   },
   "source": [
    "# Guardar dataset transformado\n",
    "transformed_file = TRANSFORMED_DATA_PATH / \"04_data_transformed.csv\"\n",
    "\n",
    "df_transformed.to_csv(transformed_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ Dataset transformado guardado en: {transformed_file}\")\n",
    "print()\n",
    "print(f\"üìä Shape final: {df_transformed.shape}\")\n",
    "print(f\"üíæ Tama√±o: {transformed_file.stat().st_size / 1024**2:.2f} MB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset transformado guardado en: ../../outputs/gian/data/04_data_transformed.csv\n",
      "\n",
      "üìä Shape final: (9701, 101)\n",
      "üíæ Tama√±o: 9.45 MB\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Reporte de Transformaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:15:42.050575Z",
     "start_time": "2026-01-19T17:15:42.041727Z"
    }
   },
   "source": [
    "# Crear reporte de transformaci√≥n\n",
    "report = {\n",
    "    \"Registros_Inicial\": len(df),\n",
    "    \"Registros_Final\": len(df_transformed),\n",
    "    \"Columnas_Inicial\": df.shape[1],\n",
    "    \"Columnas_Final\": df_transformed.shape[1],\n",
    "    \"Columnas_Agregadas\": df_transformed.shape[1] - df.shape[1],\n",
    "    \"Variables_Binarias_Encoded\": len(binary_cols),\n",
    "    \"Variables_OneHot_Encoded\": len(multi_categorical_cols),\n",
    "    \"Variables_Escaladas\": len(numeric_cols),\n",
    "    \"Variables_Sesgadas_Detectadas\": len(highly_skewed),\n",
    "}\n",
    "\n",
    "report_df = pd.DataFrame(report, index=[0]).T\n",
    "report_df.columns = [\"Valor\"]\n",
    "\n",
    "# Guardar reporte\n",
    "report_file = OUTPUT_PATH / \"reports\" / \"04_transformation_report.csv\"\n",
    "report_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "report_df.to_csv(report_file)\n",
    "\n",
    "print(\"üìã Reporte de Transformaci√≥n:\")\n",
    "print()\n",
    "print(report_df)\n",
    "print()\n",
    "print(f\"‚úÖ Reporte guardado en: {report_file}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Reporte de Transformaci√≥n:\n",
      "\n",
      "                               Valor\n",
      "Registros_Inicial               9701\n",
      "Registros_Final                 9701\n",
      "Columnas_Inicial                  67\n",
      "Columnas_Final                   101\n",
      "Columnas_Agregadas                34\n",
      "Variables_Binarias_Encoded         6\n",
      "Variables_OneHot_Encoded          17\n",
      "Variables_Escaladas               31\n",
      "Variables_Sesgadas_Detectadas     10\n",
      "\n",
      "‚úÖ Reporte guardado en: ../../outputs/gian/reports/04_transformation_report.csv\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Resumen de Transformaciones\n",
    "\n",
    "### ‚úÖ Transformaciones Aplicadas\n",
    "\n",
    "1. **Encoding de Variables Categ√≥ricas:**\n",
    "   - Label Encoding para variables binarias (Si/No, etc.)\n",
    "   - One-Hot Encoding para variables con m√∫ltiples categor√≠as\n",
    "\n",
    "2. **Escalado de Variables Num√©ricas:**\n",
    "   - StandardScaler aplicado (media=0, std=1)\n",
    "   - Variables normalizadas para el modelado\n",
    "\n",
    "3. **An√°lisis de Sesgo:**\n",
    "   - Identificadas variables con alto sesgo\n",
    "   - Candidatas para transformaci√≥n log en Feature Engineering\n",
    "\n",
    "### üìä Dataset Transformado\n",
    "\n",
    "- **Ubicaci√≥n:** \n",
    "- **Registros:** Sin p√©rdida\n",
    "- **Columnas:** Aumentadas por One-Hot Encoding\n",
    "- **Estado:** Listo para EDA y Feature Engineering\n",
    "\n",
    "### üéØ Pr√≥ximo Paso\n",
    "\n",
    "**Notebook 05: EDA (Exploratory Data Analysis)**\n",
    "- An√°lisis exploratorio profundo\n",
    "- Visualizaciones de distribuciones\n",
    "- An√°lisis de correlaciones\n",
    "- Identificaci√≥n de patrones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
