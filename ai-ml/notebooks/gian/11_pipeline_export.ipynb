{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Notebook 11: End-to-End Pipeline Export\n",
    "\n",
    "**Autor:** Gian  \n",
    "**Fecha:** 2026-01-19  \n",
    "**Objetivo:** Consolidar todos los pasos de preprocesamiento, ingenierÃ­a de caracterÃ­sticas y el modelo entrenado en un Ãºnico objeto `Pipeline` de scikit-learn exportable. Esto garantiza que el modelo pueda ser desplegado y usado para predicciones con datos crudos nuevos.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Contenido\n",
    "\n",
    "1. ConfiguraciÃ³n del entorno\n",
    "2. Carga de Datos (Crudos y Modelo)\n",
    "3. DefiniciÃ³n de Transformadores Personalizados (Feature Engineering)\n",
    "4. ConstrucciÃ³n del Pipeline\n",
    "5. ValidaciÃ³n del Pipeline (Sanity Check)\n",
    "6. Entrenamiento Final del Preprocesador\n",
    "7. ExportaciÃ³n del Pipeline Completo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. ConfiguraciÃ³n del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T23:19:18.834877Z",
     "start_time": "2026-01-19T23:19:18.606172Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "print(\"âœ… LibrerÃ­as importadas correctamente\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LibrerÃ­as importadas correctamente\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Carga de Datos y Modelo\n",
    "Necesitamos:\n",
    "1.  `data_clean.csv`: Los datos \"crudos\" originales para ajustar los scalers.\n",
    "2.  `09_best_model.pkl`: El modelo ya entrenado y optimizado."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T23:19:19.025404Z",
     "start_time": "2026-01-19T23:19:18.839220Z"
    }
   },
   "source": [
    "# Rutas\n",
    "OUTPUT_PATH = Path(\"../../outputs/gian\")\n",
    "RAW_DATA_PATH = OUTPUT_PATH / \"data\" / \"data_clean.csv\"\n",
    "MODEL_PATH = OUTPUT_PATH / \"models\" / \"09_best_model.pkl\"\n",
    "PIPELINE_PATH = OUTPUT_PATH / \"pipelines\"\n",
    "PIPELINE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cargar Datos Crudos\n",
    "df_raw = pd.read_csv(RAW_DATA_PATH)\n",
    "X_raw = df_raw.drop(columns=['cancelacion', 'cliente_id'], errors='ignore')\n",
    "y_raw = df_raw['cancelacion']\n",
    "\n",
    "# Cargar Modelo Entrenado\n",
    "trained_model = joblib.load(MODEL_PATH)\n",
    "\n",
    "print(f\"âœ… Datos crudos cargados: {X_raw.shape}\")\n",
    "print(f\"âœ… Modelo cargado: {type(trained_model).__name__}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Datos crudos cargados: (9701, 65)\n",
      "âœ… Modelo cargado: RandomForestClassifier\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. DefiniciÃ³n de Transformadores Personalizados\n",
    "Creamos clases de scikit-learn para replicar la lÃ³gica de Feature Engineering exactamente como se hizo en el Notebook 06."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T23:19:19.041772Z",
     "start_time": "2026-01-19T23:19:19.036432Z"
    }
   },
   "source": [
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ref_date=None):\n",
    "        # Si no se pasa fecha, se usarÃ¡ el momento de transformaciÃ³n (para producciÃ³n)\n",
    "        # O se puede fijar durante el fit para consistencia si es necesario.\n",
    "        self.ref_date = ref_date\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calcular la fecha de referencia mÃ¡xima del conjunto de entrenamiento si no se provee\n",
    "        if self.ref_date is None:\n",
    "             if 'ultimo_contacto_soporte' in X.columns:\n",
    "                # Asumimos que X es un DataFrame\n",
    "                # Convertir temp para sacar max sin modificar X in-place\n",
    "                temp_dates = pd.to_datetime(X['ultimo_contacto_soporte'], errors='coerce')\n",
    "                self.ref_date_ = temp_dates.max()\n",
    "        else:\n",
    "            self.ref_date_ = pd.to_datetime(self.ref_date)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Evitar modificar el original\n",
    "        X_out = X.copy()\n",
    "        \n",
    "        # 1. Intensidad de Uso\n",
    "        if 'conexiones_mensuales' in X_out.columns and 'dias_activos_semanales' in X_out.columns:\n",
    "            X_out['intensidad_uso'] = X_out['conexiones_mensuales'] / (X_out['dias_activos_semanales'] * 4 + 1)\n",
    "            \n",
    "        # 2. Ratio Carga Financiera\n",
    "        if 'cargo_mensual' in X_out.columns and 'ingresos_totales' in X_out.columns:\n",
    "            X_out['ratio_carga_financiera'] = X_out['cargo_mensual'] / (X_out['ingresos_totales'] + 1)\n",
    "            \n",
    "        # 3. Dias Desde Ultimo Contacto\n",
    "        if 'ultimo_contacto_soporte' in X_out.columns:\n",
    "            # Convertir a datetime\n",
    "            dates = pd.to_datetime(X_out['ultimo_contacto_soporte'], errors='coerce')\n",
    "            \n",
    "            # Usar fecha de referencia aprendida en fit (o la actual si se prefiere lÃ³gica dinÃ¡mica)\n",
    "            # Para reproducir el entrenamiento, usamos la ref_date del training set.\n",
    "            \n",
    "            if hasattr(self, 'ref_date_'):\n",
    "                anchor = self.ref_date_\n",
    "            else:\n",
    "                anchor = dates.max() # Fallback\n",
    "                \n",
    "            X_out['dias_desde_ultimo_contacto'] = (anchor - dates).dt.days\n",
    "            X_out['dias_desde_ultimo_contacto'] = X_out['dias_desde_ultimo_contacto'].fillna(-1)\n",
    "            \n",
    "        return X_out\n",
    "\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Seleccionar solo las columnas necesarias y en el orden correcto\n",
    "        # Verificar que existan, si no, dar error o warning (aquÃ­ asumimos happy path)\n",
    "        return X[self.columns]\n",
    "\n",
    "print(\"âœ… Clases FeatureEngineer y ColumnSelector definidas.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Clases FeatureEngineer y ColumnSelector definidas.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. ConstrucciÃ³n del Pipeline\n",
    "Ensamblamos los pasos: IngenierÃ­a -> SelecciÃ³n -> Escalado -> Modelo."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T23:19:19.059200Z",
     "start_time": "2026-01-19T23:19:19.054022Z"
    }
   },
   "source": [
    "# Cargar lista de features seleccionadas\n",
    "SELECTED_FEATURES_PATH = OUTPUT_PATH / \"reports\" / \"07_selected_features_list.csv\"\n",
    "selected_features_df = pd.read_csv(SELECTED_FEATURES_PATH)\n",
    "final_features_list = selected_features_df['Feature'].tolist()\n",
    "\n",
    "print(f\"âœ… Features seleccionadas ({len(final_features_list)}): {final_features_list}\")\n",
    "\n",
    "# Definir el Preprocesador (Sin el modelo aÃºn)\n",
    "preprocessor = Pipeline([\n",
    "    ('engineer', FeatureEngineer()), # 1. Crear variables (intensidad, ratios, dias)\n",
    "    ('selector', ColumnSelector(columns=final_features_list)), # 2. Filtrar solo las 24 Ãºtiles\n",
    "    ('scaler', StandardScaler()) # 3. Escalar (Mean=0, Std=1)\n",
    "])\n",
    "\n",
    "print(\"âœ… Pipeline de preprocesamiento definido.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Features seleccionadas (24): ['score_riesgo', 'dias_activos_semanales', 'promedio_conexion', 'conexiones_mensuales', 'caracteristicas_usadas', 'dias_ultima_conexion', 'intensidad_uso', 'tickets_soporte', 'puntuacion_nps', 'tasa_crecimiento_uso', 'puntuacion_csat', 'ratio_carga_financiera', 'tasa_apertura_email', 'errores_pago', 'antiguedad', 'ingresos_totales', 'latitud', 'cargo_mensual', 'tiempo_resolucion', 'longitud', 'codigo_postal', 'edad', 'dias_desde_ultimo_contacto', 'tiempo_sesion_promedio']\n",
      "âœ… Pipeline de preprocesamiento definido.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Entrenamiento del Preprocesador\n",
    "Ajustamos (`fit`) el preprocesador con los datos crudos (`data_clean.csv`). Esto calcularÃ¡:\n",
    "- La media y desviaciÃ³n estÃ¡ndar para el escalado.\n",
    "- La fecha de referencia mÃ¡xima para calcular antigÃ¼edades.\n",
    "\n",
    "**Nota:** No re-entrenamos el modelo. Usamos el `09_best_model.pkl` ya optimizado."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T23:19:19.086748Z",
     "start_time": "2026-01-19T23:19:19.070215Z"
    }
   },
   "source": [
    "print(\"â³ Ajustando preprocesador con datos crudos...\")\n",
    "\n",
    "# Ajustar preprocesador\n",
    "preprocessor.fit(X_raw)\n",
    "\n",
    "print(\"âœ… Preprocesador ajustado exitosamente.\")\n",
    "\n",
    "# Ensamblar el Pipeline Final (Preprocesador + Modelo)\n",
    "# Nota: 'trained_model' ya estÃ¡ entrenado, asÃ­ que el pipeline resultante estarÃ¡ listo para predecir.\n",
    "full_pipeline = Pipeline(preprocessor.steps + [('model', trained_model)])\n",
    "\n",
    "print(\"âœ… Pipeline completo ensamblado.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Ajustando preprocesador con datos crudos...\n",
      "âœ… Preprocesador ajustado exitosamente.\n",
      "âœ… Pipeline completo ensamblado.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. ValidaciÃ³n (Sanity Check)\n",
    "Verificamos que el pipeline produzca las mismas predicciones que el flujo manual anterior.\n",
    "Tomamos una muestra aleatoria, la pasamos por el pipeline y comparamos."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T23:19:19.149932Z",
     "start_time": "2026-01-19T23:19:19.096041Z"
    }
   },
   "source": [
    "# Tomar una muestra de 5 registros crudos\n",
    "sample_indices = [0, 100, 500, 1000, 5000]\n",
    "X_sample = X_raw.iloc[sample_indices].copy()\n",
    "y_true = y_raw.iloc[sample_indices]\n",
    "\n",
    "# Predecir con el pipeline\n",
    "y_pred_proba = full_pipeline.predict_proba(X_sample)[:, 1]\n",
    "y_pred_class = full_pipeline.predict(X_sample)\n",
    "\n",
    "print(\"ðŸ” Predicciones de prueba:\")\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    print(f\"   Registro {idx}: Real={y_true.iloc[i]} -> Pred={y_pred_class[i]} (Prob: {y_pred_proba[i]:.4f})\")\n",
    "\n",
    "# VerificaciÃ³n tÃ©cnica: El pipeline debe poder transformar sin errores\n",
    "try:\n",
    "    input_transformed = preprocessor.transform(X_sample)\n",
    "    print(f\"âœ… TransformaciÃ³n exitosa. Shape intermedio: {input_transformed.shape} (Debe ser 5x24)\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error en transformaciÃ³n: {e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Predicciones de prueba:\n",
      "   Registro 0: Real=0 -> Pred=0 (Prob: 0.0586)\n",
      "   Registro 100: Real=0 -> Pred=0 (Prob: 0.0405)\n",
      "   Registro 500: Real=0 -> Pred=0 (Prob: 0.0427)\n",
      "   Registro 1000: Real=0 -> Pred=1 (Prob: 0.6516)\n",
      "   Registro 5000: Real=0 -> Pred=0 (Prob: 0.1166)\n",
      "âœ… TransformaciÃ³n exitosa. Shape intermedio: (5, 24) (Debe ser 5x24)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. ExportaciÃ³n\n",
    "Guardamos el pipeline listo para producciÃ³n."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T23:19:19.189Z",
     "start_time": "2026-01-19T23:19:19.168161Z"
    }
   },
   "source": [
    "pipeline_filename = PIPELINE_PATH / \"11_production_pipeline.pkl\"\n",
    "\n",
    "joblib.dump(full_pipeline, pipeline_filename)\n",
    "\n",
    "print(f\"ðŸ’¾ Pipeline exportado a: {pipeline_filename}\")\n",
    "print(\"ðŸš€ Listo para despliegue.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Pipeline exportado a: ../../outputs/gian/pipelines/11_production_pipeline.pkl\n",
      "ðŸš€ Listo para despliegue.\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
